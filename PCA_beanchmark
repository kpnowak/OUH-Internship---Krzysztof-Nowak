import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import numpy as np

# Load dataset files
exp_data = pd.read_csv('sarcoma\\exp', delimiter='\t')
methy_data = pd.read_csv('sarcoma\\methy')
mirna_data = pd.read_csv('sarcoma\\mirna')
survival_data = pd.read_csv('sarcoma\\survival')
sarcoma_data = pd.read_csv('clinical\\sarcoma', delimiter='\t')


# Convert all columns to numeric, coercing errors to NaN
exp_data_numeric = exp_data.apply(pd.to_numeric, errors='coerce')

# Check for any missing or invalid values and clean the data
exp_data_numeric = exp_data_numeric.dropna()

print(exp_data_numeric.info())  # This will show the data types and non-null counts
print(exp_data_numeric.head())
print(exp_data.dtypes)

# Standardize the data (PCA works better with standardized data)
scaler = StandardScaler()
exp_data_scaled = scaler.fit_transform(exp_data_numeric)


def run_pca(data, n_components):
    pca = PCA(n_components=n_components)
    principal_components = pca.fit_transform(data)
    explained_variance = pca.explained_variance_ratio_
    return principal_components, explained_variance


def run_pca_iterations(data, n_runs=10):
    for run in range(n_runs):
        print(f"Run {run + 1}")
        n_components = min(data.shape[1], 10)  # Number of components can be adjusted
        pca_components, explained_variance = run_pca(data, n_components)
        
        # Print insights after each run
        print(f"Explained variance per component: {explained_variance}")
        print(f"Total variance explained: {np.sum(explained_variance)}")
        print(f"PCA Components shape: {pca_components.shape}")

        # pd.DataFrame(pca_components).to_csv(f"pca_run_{run + 1}.csv")
        
        print("-" * 50)

# Run the PCA iterations for your data
run_pca_iterations(exp_data)
